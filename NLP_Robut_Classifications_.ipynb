{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Project\n",
        "\n",
        "Authored By.\n",
        "\n",
        "**Brandyn Ewanek**\n",
        "\n",
        "*Matriculation*: 9216750\n",
        "\n",
        "*Customer ID*: 10664359\n"
      ],
      "metadata": {
        "id": "aRhS6tYj5odr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "jJ02o1tG6gxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZQxdfvS2gX7"
      },
      "outputs": [],
      "source": [
        "# general data science\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# machine learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# deep learning\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "# NLP\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data Sets"
      ],
      "metadata": {
        "id": "syk0izPN6jRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# import train and test datasets\n",
        "data_tr = fetch_20newsgroups(subset='train')\n",
        "data_ts = fetch_20newsgroups(subset='test')\n",
        "data_tr"
      ],
      "metadata": {
        "id": "SzU2v6GZ6bwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the labels (target) and target names\n",
        "labels = data_tr.target\n",
        "label_names = data_tr.target_names\n",
        "\n",
        "# Create a countplot of the label distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(y=labels)\n",
        "plt.title('Distribution of 20 Newsgroups Categories in Training')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Category Index')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V7TRDuIK7jLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a countplot of the label distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(y=labels)\n",
        "plt.title('Distribution of 20 Newsgroups Categories in Training')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Category Name')\n",
        "\n",
        "# Create a mapping from index to category name\n",
        "index_to_name = {i: name for i, name in enumerate(label_names)}\n",
        "\n",
        "# Set the y-axis tick labels to the category names\n",
        "plt.yticks(list(index_to_name.keys()), list(index_to_name.values()))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2v2Ff9IU8VnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ Roughly uniform distribution with about 600 in each and only a few categories deviate from this, religion.misc about 375 and politics.misc about 450 and alt.atheis about 500 are the under represented categories.  \n"
      ],
      "metadata": {
        "id": "deTX9FH6958U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the labels and target names\n",
        "labels = data_ts.target\n",
        "label_names = data_ts.target_names\n",
        "\n",
        "# Create a countplot of the label distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(y=labels)\n",
        "plt.title('Distribution of 20 Newsgroups Categories in Test')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Category Name')\n",
        "\n",
        "# Create a mapping from index to category name\n",
        "index_to_name = {i: name for i, name in enumerate(label_names)}\n",
        "\n",
        "# Set the y-axis tick labels to the category names\n",
        "plt.yticks(list(index_to_name.keys()), list(index_to_name.values()))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUxqdvp4--f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ Distribution of classes is remarkably similar in target dataset."
      ],
      "metadata": {
        "id": "l4z5bHYl_gix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Bubble of New Articles"
      ],
      "metadata": {
        "id": "KexmuZbs_7VU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5094082d"
      },
      "source": [
        "# Concatenate all the text documents from the training data into a single string.\n",
        "all_text = \" \".join(data_tr.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def clean_text"
      ],
      "metadata": {
        "id": "QriXfzr_hPlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by converting to lowercase, removing punctuation,\n",
        "    and removing stop words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Removed the split and join that was adding spaces between chars\n",
        "    return text"
      ],
      "metadata": {
        "id": "bsYdaHf9AgcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(all_text)"
      ],
      "metadata": {
        "id": "bIIa6OUxAirp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "id": "CgmqZzIuidmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(cleaned_text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3xmor43wAjtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Bubble without Stopwords"
      ],
      "metadata": {
        "id": "ELWwIwE_BHYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "# Access the stopwords from the module\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "id": "4UxzdCrMBlV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by converting to lowercase, removing punctuation,\n",
        "    and removing stop words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    cleaned_words = [word for word in text.split() if word not in stopwords]\n",
        "    return \" \".join(cleaned_words)"
      ],
      "metadata": {
        "id": "LVzAJnKjBftB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18fea3cb"
      },
      "source": [
        "cleaned_text = clean_text(all_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "223deb71"
      },
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(cleaned_text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ With the stop words removed the word cloud shows a much clearer representation of the key topics and relevant terms in the dataset.\n",
        "+ Common words like \"the,\" \"a,\" \"is,\" etc., which don't carry significant meaning for topic identification, have been removed, allowing more meaningful words such as those related to\n",
        "  + technology (windows, graphics, cars, bit),\n",
        "  + science (science, machine, based, arguement), and\n",
        "  + religion (god, reason)\n",
        "  \n",
        "  stand out more now."
      ],
      "metadata": {
        "id": "e9Z5ilK-CpG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "jD1Upa91DtsR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b983f5a8"
      },
      "source": [
        "## Define preprocessing function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b050d151"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "# Download the Porter Stemmer\n",
        "nltk.download('porter_stemmer')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses the input text by converting to lowercase,\n",
        "    removing punctuation, removing stop words, and applying stemming.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if word not in stopwords]\n",
        "    stemmed_words = [stemmer.stem(word) for word in cleaned_words]\n",
        "    return \" \".join(stemmed_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a00c6169"
      },
      "source": [
        "**Porter Stemmer**\n",
        "\n",
        "The Porter Stemmer is an algorithm for removing suffixes from words in English. Its main purpose is to reduce words to their root form, which is called a stem. This is useful in Natural Language Processing (NLP) tasks because it helps to group together different forms of the same word (e.g., \"running,\" \"runs,\" and \"ran\" would all be reduced to the stem \"run\"). This can improve the performance of text analysis and machine learning models by reducing the size of the vocabulary and focusing on the core meaning of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c88522a"
      },
      "source": [
        "# Apply the preprocessing function to the training and testing data\n",
        "preprocessed_train_data = [preprocess_text(doc) for doc in data_tr.data]\n",
        "preprocessed_test_data = [preprocess_text(doc) for doc in data_ts.data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01caf533"
      },
      "source": [
        "#  Import TfidfVectorizer and transform the preprocessed training and testing data into TF-IDF features.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_train_features = tfidf_vectorizer.fit_transform(preprocessed_train_data)\n",
        "tfidf_test_features = tfidf_vectorizer.transform(preprocessed_test_data)\n",
        "\n",
        "print(\"Shape of TF-IDF training features:\", tfidf_train_features.shape)\n",
        "print(\"Shape of TF-IDF testing features:\", tfidf_test_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1c22a0"
      },
      "source": [
        "\n",
        "*   The resulting TF-IDF features for the training data have a shape of (11314, 119712), and the testing data have a shape of (7532, 119712).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "C1-WAWbGm7Yp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36447d44"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "511d356e"
      },
      "source": [
        "# Train a Logistic Regression model\n",
        "log_reg_model = LogisticRegression()\n",
        "log_reg_model.fit(tfidf_train_features, data_tr.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def print scores"
      ],
      "metadata": {
        "id": "-L6gRrSqnsBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_scores(model):\n",
        "    \"\"\"\n",
        "    Prints the accuracy, precision, recall, and F1 score of a trained model.\n",
        "    \"\"\"\n",
        "    # Make predictions on the train data\n",
        "    log_reg_predictions = model.predict(tfidf_train_features)\n",
        "\n",
        "    # train performance metrics\n",
        "    accuracy_tr = accuracy_score(data_tr.target, log_reg_predictions)\n",
        "    precision_tr = precision_score(data_tr.target, log_reg_predictions, average='weighted')\n",
        "    recall_tr = recall_score(data_tr.target, log_reg_predictions, average='weighted')\n",
        "    f1_tr = f1_score(data_tr.target, log_reg_predictions, average='weighted')\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    log_reg_predictions = model.predict(tfidf_test_features)\n",
        "\n",
        "    # test performance metrics\n",
        "    accuracy_ts = accuracy_score(data_ts.target, log_reg_predictions)\n",
        "    precision_ts = precision_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "    recall_ts = recall_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "    f1_ts = f1_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "\n",
        "    # Print the performance metrics\n",
        "    print(f\"Model Train Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_tr:.4f}\")\n",
        "    print(f\"Precision: {precision_tr:.4f}\")\n",
        "    print(f\"Recall: {recall_tr:.4f}\")\n",
        "    print(f\"F1-score: {f1_tr:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Print the performance metrics\n",
        "    print(f\"Model Test Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_ts:.4f}\")\n",
        "    print(f\"Precision: {precision_ts:.4f}\")\n",
        "    print(f\"Recall: {recall_ts:.4f}\")\n",
        "    print(f\"F1-score: {f1_ts:.4f}\")"
      ],
      "metadata": {
        "id": "nOe_0nV1nu4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53cf7765"
      },
      "source": [
        "# Make predictions on the test data\n",
        "log_reg_predictions = log_reg_model.predict(tfidf_test_features)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy_log = accuracy_score(data_ts.target, log_reg_predictions)\n",
        "precision_log = precision_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "recall_log = recall_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "f1_log = f1_score(data_ts.target, log_reg_predictions, average='weighted')\n",
        "\n",
        "# Print the performance metrics\n",
        "print_scores(log_reg_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ The Logistic Regression Model is showing signs of overfitting, as seen by the .14 difference in scores"
      ],
      "metadata": {
        "id": "1RwAUhN0q7mJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580a9e8a"
      },
      "source": [
        "## Decision tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "786b8778"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate a DecisionTreeClassifier\n",
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the Decision Tree model\n",
        "dt_model.fit(tfidf_train_features, data_tr.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82fdfa7"
      },
      "source": [
        "# Make predictions on the test data using the trained Decision Tree model\n",
        "dt_predictions = dt_model.predict(tfidf_test_features)\n",
        "\n",
        "# Calculate performance metrics for the Decision Tree model\n",
        "accuracy_dt = accuracy_score(data_ts.target, dt_predictions)\n",
        "precision_dt = precision_score(data_ts.target, dt_predictions, average='weighted')\n",
        "recall_dt = recall_score(data_ts.target, dt_predictions, average='weighted')\n",
        "f1_dt = f1_score(data_ts.target, dt_predictions, average='weighted')\n",
        "\n",
        "# Print the performance metrics\n",
        "print_scores(dt_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ The Decision Tree Model is significantly overfitting as seen but the .45 difference in scores between train and test"
      ],
      "metadata": {
        "id": "NkxZARchrLRP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ed4d71"
      },
      "source": [
        "## Random forest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9db45a60"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model.fit(tfidf_train_features, data_tr.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d547df9b"
      },
      "source": [
        "# Make predictions on the test data using the trained Random Forest model\n",
        "rf_predictions = rf_model.predict(tfidf_test_features)\n",
        "\n",
        "# Calculate performance metrics for the Random Forest model\n",
        "accuracy_rf = accuracy_score(data_ts.target, rf_predictions)\n",
        "precision_rf = precision_score(data_ts.target, rf_predictions, average='weighted')\n",
        "recall_rf = recall_score(data_ts.target, rf_predictions, average='weighted')\n",
        "f1_rf = f1_score(data_ts.target, rf_predictions, average='weighted')\n",
        "\n",
        "# Print the performance metrics\n",
        "print_scores(rf_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ The Random Forest Classifier is also showing signs of overfitting seen by the 0.23 difference in train and test scores."
      ],
      "metadata": {
        "id": "OifYRZmPscef"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827574ad"
      },
      "source": [
        "## Gradient boosting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7facae8a"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Instantiate a GradientBoostingClassifier object\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "gb_model.fit(tfidf_train_features, data_tr.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "+ GradientBoosting took a very long time to train, this related to speed of production and could be a barrier to using this model in production.\n",
        "+"
      ],
      "metadata": {
        "id": "f5nmTlitwNin"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f36f8c8a"
      },
      "source": [
        "# Use the trained gb_model to make predictions on the tfidf_test_features\n",
        "gb_predictions = gb_model.predict(tfidf_test_features)\n",
        "\n",
        "# Calculate performance metrics for the Gradient Boosting model\n",
        "accuracy_gb = accuracy_score(data_ts.target, gb_predictions)\n",
        "precision_gb = precision_score(data_ts.target, gb_predictions, average='weighted')\n",
        "recall_gb = recall_score(data_ts.target, gb_predictions, average='weighted')\n",
        "f1_gb = f1_score(data_ts.target, gb_predictions, average='weighted')\n",
        "\n",
        "# Print the performance metrics\n",
        "print_scores(gb_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18fbee07"
      },
      "source": [
        "## Summarize scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d70713f"
      },
      "source": [
        "Creating a dictionary to store the performance metrics for each model and then convert it into a pandas DataFrame for a summary table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb99b858"
      },
      "source": [
        "# Store performance metrics in a dictionary\n",
        "performance_metrics = {\n",
        "    'Logistic Regression': {\n",
        "        'Accuracy': accuracy_log,\n",
        "        'Precision': precision_log,\n",
        "        'Recall': precision_log,\n",
        "        'F1-score': f1_log\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'Accuracy': accuracy_dt,\n",
        "        'Precision': precision_dt,\n",
        "        'Recall': recall_dt,\n",
        "        'F1-score': f1_dt\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'Accuracy': accuracy_dt,\n",
        "        'Precision': accuracy_dt,\n",
        "        'Recall': recall_dt,\n",
        "        'F1-score': recall_dt\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'Accuracy': accuracy_gb,\n",
        "        'Precision': precision_gb,\n",
        "        'Recall': recall_gb,\n",
        "        'F1-score': f1_gb\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a pandas DataFrame\n",
        "performance_df = pd.DataFrame.from_dict(performance_metrics, orient='index')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(performance_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ba4007"
      },
      "source": [
        "### Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Logistic Regression model achieved the highest performance across all metrics: Accuracy (0.8307), Precision (0.8341), Recall (0.8307), and F1-score (0.8295).\n",
        "*   The Decision Tree model showed the lowest performance with an Accuracy of 0.5613 and an F1-score of 0.5631.\n",
        "*   Ensemble methods (Random Forest and Gradient Boosting) performed better than the single Decision Tree, with Random Forest having an Accuracy of 0.7768 and Gradient Boosting an Accuracy of 0.7434.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Logistic Regression appears to be the most suitable model for this dataset based on the evaluated metrics.\n",
        "*   Further hyperparameter tuning for the Random Forest model could potentially improve performance.\n",
        " *  NOTE that Gradient Boosting because of its sequential nature is too computationally heavy for this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning Hyperparameters - Random Forest"
      ],
      "metadata": {
        "id": "W2mT0AnYu7nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# array of 60 to 300 estimators\n",
        "estimators = np.arange(60,300, 20)\n",
        "\n",
        "train_scores_rf = []\n",
        "test_scores_rf = []\n",
        "\n",
        "# interate through estimators\n",
        "for e in estimators:\n",
        "  rf = RandomForestClassifier(n_estimators=e).fit(tfidf_train_features, data_tr.target)\n",
        "  train_scores_rf.append(rf.score(tfidf_train_features, data_tr.target))\n",
        "  test_scores_rf.append(rf.score(tfidf_test_features, data_ts.target))\n",
        "\n",
        "# plot lineplots of train and test scores\n",
        "sns.lineplot(x=estimators, y=train_scores_rf, label='Train Score rand frst', color='#193A40', alpha=.42)\n",
        "sns.lineplot(x=estimators, y=test_scores_rf, label='Test Score  rand frst', color='#193A40')\n",
        "\n",
        "# get best estimator based on test score\n",
        "best_randforest = np.round(estimators[np.argmax(test_scores_rf)],2)\n",
        "\n",
        "# add title and legend\n",
        "plt.title(f'Best Estimators Random Forest: {best_randforest}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GW8iDQqOu_yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array of max depth\n",
        "max_depths = np.arange(2,20)\n",
        "\n",
        "train_scores_rf = []\n",
        "test_scores_rf = []\n",
        "\n",
        "# interate through max_depths\n",
        "for m in max_depths:\n",
        "  rf = RandomForestClassifier(n_estimators=260, max_depth=m).fit(tfidf_train_features, data_tr.target)\n",
        "  train_scores_rf.append(rf.score(tfidf_train_features, data_tr.target))\n",
        "  test_scores_rf.append(rf.score(tfidf_test_features, data_ts.target))\n",
        "\n",
        "# plot lineplots of train and test scores\n",
        "sns.lineplot(x=max_depths, y=train_scores_rf, label='Train Score rand frst', color='#193A40', alpha=.42)\n",
        "sns.lineplot(x=max_depths, y=test_scores_rf, label='Test Score  rand frst', color='#193A40')\n",
        "\n",
        "# get best estimator based on test score\n",
        "best_randforest = np.round(max_depths[np.argmax(test_scores_rf)],2)\n",
        "\n",
        "# add title and legend\n",
        "plt.title(f'Best max_depth Random Forest: {best_randforest}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OW94TUq0FFLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array of max depth\n",
        "min_samples_splits = np.arange(2,20)\n",
        "\n",
        "train_scores_rf = []\n",
        "test_scores_rf = []\n",
        "\n",
        "# interate through estimators\n",
        "for m in min_samples_splits:\n",
        "  rf = RandomForestClassifier(n_estimators=260, max_depth=13, min_samples_split=m).fit(tfidf_train_features, data_tr.target)\n",
        "  train_scores_rf.append(rf.score(tfidf_train_features, data_tr.target))\n",
        "  test_scores_rf.append(rf.score(tfidf_test_features, data_ts.target))\n",
        "\n",
        "# plot lineplots of train and test scores\n",
        "sns.lineplot(x=min_samples_splits, y=train_scores_rf, label='Train Score rand frst', color='#193A40', alpha=.42)\n",
        "sns.lineplot(x=min_samples_splits, y=test_scores_rf, label='Test Score  rand frst', color='#193A40')\n",
        "\n",
        "# get best estimator based on test score\n",
        "best_randforest = np.round(min_samples_splits[np.argmax(test_scores_rf)],2)\n",
        "\n",
        "# add title and legend\n",
        "plt.title(f'Best min_samples_split Random Forest: {best_randforest}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1VrimPG2FcwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=260,\n",
        "                                  max_depth=13,\n",
        "                                  min_samples_split=10)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model.fit(tfidf_train_features, data_tr.target)\n",
        "\n",
        "# Print the performance metrics\n",
        "print_scores(rf_model)"
      ],
      "metadata": {
        "id": "UJBMBBf8Yr1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate performance metrics for the Random Forest model\n",
        "accuracy_rf1 = accuracy_score(data_ts.target, rf_predictions)\n",
        "precision_rf1 = precision_score(data_ts.target, rf_predictions, average='weighted')\n",
        "recall_rf1 = recall_score(data_ts.target, rf_predictions, average='weighted')\n",
        "f1_rf1 = f1_score(data_ts.target, rf_predictions, average='weighted')\n"
      ],
      "metadata": {
        "id": "tnK7-ptlcIw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store performance metrics in a dictionary\n",
        "performance_metrics = {\n",
        "    'Logistic Regression': {\n",
        "        'Accuracy': accuracy_log,\n",
        "        'Precision': precision_log,\n",
        "        'Recall': precision_log,\n",
        "        'F1-score': f1_log\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'Accuracy': accuracy_dt,\n",
        "        'Precision': precision_dt,\n",
        "        'Recall': recall_dt,\n",
        "        'F1-score': f1_dt\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'Accuracy': accuracy_dt,\n",
        "        'Precision': accuracy_dt,\n",
        "        'Recall': recall_dt,\n",
        "        'F1-score': recall_dt\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'Accuracy': accuracy_gb,\n",
        "        'Precision': precision_gb,\n",
        "        'Recall': recall_gb,\n",
        "        'F1-score': f1_gb\n",
        "    },\n",
        "    'Random Forest Tuned': {\n",
        "        'Accuracy': accuracy_rf1,\n",
        "        'Precision': accuracy_rf1,\n",
        "        'Recall': recall_rf1,\n",
        "        'F1-score': recall_rf1\n",
        "    },\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a pandas DataFrame\n",
        "performance_df = pd.DataFrame.from_dict(performance_metrics, orient='index')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(performance_df)"
      ],
      "metadata": {
        "id": "0IfEHnBNcPTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranformers - distilbert base uncased"
      ],
      "metadata": {
        "id": "MT4wIi6CJBr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "X8BN8ZA_M8PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the zero-shot-classification pipeline with the BART model\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1 # Use GPU if available, otherwise CPU\n",
        "classifier = transformers.pipeline(\"zero-shot-classification\", model=\"distilbert-base-uncased\", device=device)"
      ],
      "metadata": {
        "id": "3mdP8Xj8JGP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the zero-shot-classification pipeline\n",
        "# The pipeline expects a list of inputs and candidate labels\n",
        "transformer_predictions_raw = classifier(data_ts.data, candidate_labels=data_ts.target_names)\n",
        "\n",
        "# Extract the predicted label (category name) for each sample\n",
        "transformer_predicted_labels = [pred['labels'][0] for pred in transformer_predictions_raw]\n",
        "\n",
        "# Map the predicted label names back to the original target indices\n",
        "# This requires creating a mapping from target name to index\n",
        "name_to_index = {name: i for i, name in enumerate(data_ts.target_names)}\n",
        "transformer_predicted_indices = [name_to_index[label] for label in transformer_predicted_labels]"
      ],
      "metadata": {
        "id": "3bYstLc_NMdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e538d7c"
      },
      "source": [
        "# Calculate performance metrics for the transformer model\n",
        "accuracy_transformer = accuracy_score(data_ts.target, transformer_predicted_indices)\n",
        "precision_transformer = precision_score(data_ts.target, transformer_predicted_indices, average='weighted')\n",
        "recall_transformer = recall_score(data_ts.target, transformer_predicted_indices, average='weighted')\n",
        "f1_transformer = f1_score(data_ts.target, transformer_predicted_indices, average='weighted')\n",
        "\n",
        "performance_metrics = {}\n",
        "\n",
        "# Add transformer performance metrics to the dictionary\n",
        "performance_metrics['Transformer'] = {\n",
        "    'Accuracy': accuracy_transformer,\n",
        "    'Precision': precision_transformer,\n",
        "    'Recall': recall_transformer,\n",
        "    'F1-score': f1_transformer\n",
        "}\n",
        "\n",
        "# Convert the updated dictionary to a pandas DataFrame\n",
        "performance_df = pd.DataFrame.from_dict(performance_metrics, orient='index')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(performance_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Did quite poorly with no traing"
      ],
      "metadata": {
        "id": "Xzpmwh3YvQDD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8u3OxQvCvNm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Transformer"
      ],
      "metadata": {
        "id": "79k8mJjipfHQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04093227"
      },
      "source": [
        "### prepare dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1f350ee"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Create a Hugging Face Dataset dictionary\n",
        "newsgroups_dataset = {\n",
        "    'train': Dataset.from_dict({'data': data_tr.data, 'target': data_tr.target}),\n",
        "    'test': Dataset.from_dict({'data': data_ts.data, 'target': data_ts.target})\n",
        "}\n",
        "\n",
        "print(newsgroups_dataset['train'])\n",
        "print(newsgroups_dataset['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_dataset"
      ],
      "metadata": {
        "id": "GSVkPMFGzwnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ce09dfa"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"data\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to the datasets\n",
        "tokenized_datasets = {\n",
        "    'train': newsgroups_dataset['train'].map(tokenize_function, batched=True),\n",
        "    'test': newsgroups_dataset['test'].map(tokenize_function, batched=True)\n",
        "}\n",
        "\n",
        "print(tokenized_datasets['train'])\n",
        "print(tokenized_datasets['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "bE5bjodqzylr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# First, determine the number of unique labels in your dataset\n",
        "num_labels = len(set(newsgroups_dataset['train']['target']))\n",
        "\n",
        "# Load the model with the correct number of labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels\n",
        ")"
      ],
      "metadata": {
        "id": "PMV4K8iWwsao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "lAYvmNe-z-vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "k9VOWGY3x4ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "#  Load all the metrics you want to use\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "#  Define the function to compute the metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    # Unpack the predictions and true labels\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Get the predicted class by finding the index of the highest logit\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    # The 'average=\"weighted\"' parameter is important for multi-class classification\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "\n",
        "    # Return a dictionary of the metric names and their values\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"precision\": precision[\"precision\"],\n",
        "        \"recall\": recall[\"recall\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "qx3VnPi-xy2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "55vAqn8O0DZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Trainer expects the label column to be named 'labels'\n",
        "tokenized_datasets['train'] = tokenized_datasets['train'].rename_column(\"target\", \"labels\")\n",
        "tokenized_datasets['test'] = tokenized_datasets['test'].rename_column(\"target\", \"labels\")"
      ],
      "metadata": {
        "id": "sa6RyAx8x2gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "la38f2BC09CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "tNulHgJRioop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",           # Directory to save the model\n",
        "    eval_strategy=\"epoch\",                   # Evaluate performance at the end of each epoch\n",
        "    learning_rate=2e-5,                      # The learning rate\n",
        "    per_device_train_batch_size=16,          # Batch size for training\n",
        "    per_device_eval_batch_size=16,           # Batch size for evaluation\n",
        "    num_train_epochs=4,                      # Number of times to go through the training data\n",
        "    weight_decay=0.01,                       # Strength of weight decay regularization\n",
        "    push_to_hub=False,                       # Set to True if you want to upload to Hugging Face Hub\n",
        ")"
      ],
      "metadata": {
        "id": "2xUvkRVU0SPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                             # The instantiated model to be trained\n",
        "    args=training_args,                      # Training arguments you defined\n",
        "    train_dataset=tokenized_datasets[\"train\"], # The training dataset\n",
        "    eval_dataset=tokenized_datasets[\"test\"],   # The evaluation/test dataset\n",
        "    tokenizer=tokenizer,                     # The tokenizer (good practice to include)\n",
        "    compute_metrics=compute_metrics,         # Your function to calculate metrics\n",
        ")"
      ],
      "metadata": {
        "id": "GQbjbRzPzBg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hvEqQip9UuAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/03. Resources/03. Education/39 Bachelors IU/19. NLP Project/classification_model\")"
      ],
      "metadata": {
        "id": "ghgZy3vCUUSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This command starts the fine-tuning process.\n",
        "# It will print out the training loss and evaluation metrics at each epoch.\n",
        "trainer.train()\n",
        "\n",
        "# After training is complete, run a final evaluation on the test set\n",
        "# to get the final performance metrics.\n",
        "final_results = trainer.evaluate()\n",
        "\n",
        "print(\"--- Final Evaluation Results ---\")\n",
        "print(final_results)"
      ],
      "metadata": {
        "id": "eT5XDyTSzGKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "Although we see loss increase on the validation set.  the accuracy, precision and recall and F1 are all continuing to increase.  This could be overfitting to to the higher represented classes.  "
      ],
      "metadata": {
        "id": "bNYsjvZcQwIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "6jteBo-EJI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "mz6nSMAZwLOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER Sentiment Intensity Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_vader(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of a given text using VADER and returns the label and score.\n",
        "    VADER doesn't have strict length limits, so no truncation is needed.\n",
        "    \"\"\"\n",
        "    # Get sentiment scores\n",
        "    sentiment_dict = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Determine sentiment label based on compound score\n",
        "    compound_score = sentiment_dict['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        label = 'POSITIVE'\n",
        "    elif compound_score <= -0.05:\n",
        "        label = 'NEGATIVE'\n",
        "    else:\n",
        "        label = 'NEUTRAL'\n",
        "\n",
        "    # Use the compound score as the overall sentiment score\n",
        "    score = compound_score\n",
        "\n",
        "    return label, score\n",
        "\n",
        "# # Create a DataFrame with the news data\n",
        "df = pd.DataFrame({\n",
        "    'text': data_tr.data,\n",
        "    'category': [data_tr.target_names[i] for i in data_tr.target]\n",
        "})\n",
        "\n",
        "\n",
        "# Get sentiment for each news article using VADER\n",
        "df['sentiment_label'], df['sentiment_score'] = zip(*df['text'].apply(get_sentiment_vader))\n"
      ],
      "metadata": {
        "id": "U69H-DPju5KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of sentiment per category\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.countplot(y='category', hue='sentiment_label', data=df, palette={'POSITIVE': 'green', 'NEGATIVE': 'red', 'NEUTRAL': 'blue'})\n",
        "plt.title('Sentiment Distribution per News Category')\n",
        "plt.xlabel('Number of Articles')\n",
        "plt.ylabel('News Category')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmG-RmA2vTVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the aesthetic style of the plots\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Create a FacetGrid to plot histograms for each category\n",
        "g = sns.FacetGrid(df, col=\"category\", col_wrap=4, height=3, aspect=1.5, sharex=True, sharey=False)\n",
        "\n",
        "# Map a histogram plot onto each facet\n",
        "g.map(sns.histplot, \"sentiment_score\", bins=15, kde=True, color='skyblue')\n",
        "\n",
        "# Adjust the titles for each subplot\n",
        "g.set_titles(\"{col_name}\", size=10)\n",
        "\n",
        "# Add an overall title for the entire plot\n",
        "plt.suptitle('Distribution of Sentiment Scores by News Category', y=1.02, size=16)\n",
        "\n",
        "# Improve layout\n",
        "g.fig.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sQd9LXUlSPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sentiment = df.groupby('category')['sentiment_score'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average Sentiment Score by Category:\")\n",
        "print(avg_sentiment)\n"
      ],
      "metadata": {
        "id": "tYQhjzANTiv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Classification News System"
      ],
      "metadata": {
        "id": "dezfmAwVJNVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-generativeai==0.5.*"
      ],
      "metadata": {
        "id": "qj6tQlEKbWJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF pillow  vaderSentiment"
      ],
      "metadata": {
        "id": "RM8GiqspJMwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qurQL8iZ0-qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "# --- Configuration and Model Loading ---\n",
        "genai.configure(api_key='AIzaSyDQfiKjfkAC6lXuslTug_AUGoBFllyEzPI')\n",
        "\n",
        "\n",
        "# The dictionary of average sentiment scores\n",
        "avg_sentiment_by_category = {\n",
        "    'comp.graphics': 0.609412,'misc.forsale': 0.541270,\n",
        "    'rec.sport.baseball': 0.517947,'rec.sport.hockey': 0.515291,\n",
        "    'comp.sys.ibm.pc.hardware': 0.482741,'sci.electronics': 0.477511,\n",
        "    'sci.space': 0.443316,'comp.os.ms-windows.misc': 0.433856,\n",
        "    'soc.religion.christian': 0.414938,'comp.sys.mac.hardware': 0.414498,\n",
        "    'comp.windows.x': 0.397715,'rec.motorcycles': 0.368997,\n",
        "    'sci.crypt': 0.366228,'rec.autos': 0.324439,\n",
        "    'sci.med': 0.230119,'talk.religion.misc': 0.228395,\n",
        "    'alt.atheism': 0.150629,'talk.politics.misc': 0.117374,\n",
        "    'talk.politics.mideast': -0.284440,'talk.politics.guns': -0.323723\n",
        "}\n",
        "\n",
        "# predicted to name dict\n",
        "label_to_category = {\n",
        "    0: 'alt.atheism', 1: 'comp.graphics',\n",
        "    2: 'comp.os.ms-windows.misc',3: 'comp.sys.ibm.pc.hardware',\n",
        "    4: 'comp.sys.mac.hardware',5: 'comp.windows.x',\n",
        "    6: 'misc.forsale',7: 'rec.autos',\n",
        "    8: 'rec.motorcycles',9: 'rec.sport.baseball',\n",
        "    10: 'rec.sport.hockey',11: 'sci.crypt',\n",
        "    12: 'sci.electronics',13: 'sci.med',\n",
        "    14: 'sci.space',15: 'soc.religion.christian',\n",
        "    16: 'talk.politics.guns',17: 'talk.politics.mideast',\n",
        "    18: 'talk.politics.misc',19: 'talk.religion.misc'\n",
        "}\n",
        "\n",
        "# --- Load models  ---\n",
        "category_predictor = pipeline(\n",
        "    \"text-classification\",\n",
        "    model='/content/drive/MyDrive/03. Resources/03. Education/39 Bachelors IU/19. NLP Project/classification_model'\n",
        ")\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# --- VADER Sentiment Analysis Function ---\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"\n",
        "    Analyzes sentiment using VADER and returns label and normalized score.\n",
        "    \"\"\"\n",
        "    sentiment_dict = sentiment_analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment_dict['compound']\n",
        "\n",
        "    # Convert VADER's -1 to +1 scale to 0-1 scale for consistency\n",
        "    normalized_score = (compound_score + 1) / 2\n",
        "\n",
        "    # Determine label based on VADER's standard thresholds\n",
        "    if compound_score >= 0.05:\n",
        "        label = 'POSITIVE'\n",
        "    elif compound_score <= -0.05:\n",
        "        label = 'NEGATIVE'\n",
        "    else:\n",
        "        label = 'NEUTRAL'\n",
        "\n",
        "    return label, normalized_score\n",
        "\n",
        "# --- The Analysis Function ---\n",
        "def analyze_document(text_content):\n",
        "    \"\"\"\n",
        "    Analyzes text for category, sentiment, and provides an AI explanation for sentiment deviation.\n",
        "    Only accepts string input.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not isinstance(text_content, str):\n",
        "        raise ValueError(\"Input must be a string of text.\")\n",
        "\n",
        "    # Clean and check text\n",
        "    text_content = text_content.strip()\n",
        "    if not text_content:\n",
        "        return \"Input text is empty.\"\n",
        "\n",
        "    # --- Category Prediction ---\n",
        "    category_prediction = category_predictor(text_content)[0]\n",
        "    label = category_prediction['label']\n",
        "\n",
        "    # Convert LABEL_1 to just the number, then map to category name\n",
        "    label_number = int(label.replace('LABEL_', ''))\n",
        "    predicted_category = label_to_category.get(label_number, f\"Unknown category ({label})\")\n",
        "\n",
        "    # --- Sentiment Analysis ---\n",
        "    sentiment_label, sentiment_score = analyze_sentiment_vader(text_content)\n",
        "\n",
        "    # --- Comparison with Category Average ---\n",
        "    avg_score = avg_sentiment_by_category.get(predicted_category)\n",
        "    if avg_score is not None:\n",
        "        deviation = sentiment_score - avg_score\n",
        "        comparison = \"more positive\" if deviation > 0 else \"more negative\"\n",
        "        deviation_text = f\"This is {abs(deviation):.3f} {comparison} than the average for this category.\"\n",
        "        avg_score_display = f\"{avg_score:.3f}\"\n",
        "    else:\n",
        "        deviation_text = \"Could not find an average sentiment to compare.\"\n",
        "        avg_score_display = \"N/A\"\n",
        "\n",
        "    # --- AI Explanation for Deviation ---\n",
        "    explanation = \"No deviation analysis performed.\"\n",
        "    if avg_score is not None and abs(deviation) > 0.01:  # Only analyze if deviation is meaningful\n",
        "        try:\n",
        "            text_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "            prompt = f\"\"\"\n",
        "            Analyze the following text from the '{predicted_category}' category.\n",
        "            The text's sentiment score is {sentiment_score:.3f}, while the category average is {avg_score:.3f}.\n",
        "            This is a deviation of {deviation:.3f}.\n",
        "\n",
        "            Based on the content of the text below, provide a brief, 2-sentence explanation for the likely cause of this sentiment deviation.\n",
        "            For example, does it use strong emotional language, discuss a controversial topic, or express a particularly one-sided viewpoint?\n",
        "\n",
        "            TEXT:\n",
        "            \"{text_content[:2000]}\"\n",
        "            \"\"\"\n",
        "            response = text_model.generate_content(prompt)\n",
        "            explanation = response.text.strip()\n",
        "        except Exception as e:\n",
        "            explanation = f\"Could not generate AI explanation due to an error: {e}\"\n",
        "\n",
        "    # --- Format the final output ---\n",
        "    output = (\n",
        "        f\"--- Analysis Report ---\\n\"\n",
        "        f\" **Predicted Category:** {predicted_category}\\n\"\n",
        "        f\" **Sentiment:** {sentiment_label} (Score: {sentiment_score:.3f})\\n\"\n",
        "        f\" **Category Average Sentiment:** {avg_score_display}\\n\"\n",
        "        f\" **Comparison:** {deviation_text}\\n\"\n",
        "        f\" **AI Explanation:** {explanation}\"\n",
        "    )\n",
        "    return output"
      ],
      "metadata": {
        "id": "rzwygesa54jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "\n",
        "# Example 1: A more negative article in a typically positive category\n",
        "my_text = (\"Subject: Mac Performance Issues\\n\\n\"\n",
        "           \"I just upgraded to the latest OS and my Mac is now incredibly slow. \"\n",
        "           \"The battery life has been terrible, and applications keep crashing. \"\n",
        "           \"I am extremely disappointed with this update, it has ruined my workflow.\")\n",
        "\n",
        "print(analyze_document(my_text))"
      ],
      "metadata": {
        "id": "BwYlQb8R6uw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\n",
        "    # --- 1. Low Deviation (Straightforward examples) ---\n",
        "    (\"Subject: New Graphics Card\\n\\nJust installed the latest GPU and the performance is incredible. \"\n",
        "     \"Getting over 120 FPS on all modern titles at max settings. A huge leap from my old card.\"),\n",
        "\n",
        "    (\"Subject: Re: Is there a God?\\n\\nI believe that faith is a personal journey. \"\n",
        "     \"The evidence for a higher power can be seen in the complexity and beauty of the universe. \"\n",
        "     \"It's a source of great comfort and strength for many people.\"),\n",
        "\n",
        "    # --- 2. Hard to Classify (Mixing topics) ---\n",
        "    (\"Subject: Encryption for PC hardware?\\n\\nI'm looking for a way to encrypt the data on my IBM compatible PC's main drive. \"\n",
        "     \"Are there any hardware-level solutions, or should I rely on OS-level software like BitLocker? \"\n",
        "     \"Security is my main concern here, especially against government snooping.\"),\n",
        "\n",
        "    (\"Subject: Selling my old Mac and Honda Civic\\n\\nClearing out some old tech and my car. \"\n",
        "     \"I have a 2018 MacBook Pro in great condition and a 2015 Honda Civic with 80,000 miles. \"\n",
        "     \"Both run perfectly. DM for prices and more details. For sale in the Bay Area.\"),\n",
        "\n",
        "    # --- 3. Different Sentiment (Opposite of category average) ---\n",
        "    (\"Subject: Disappointed with the new car\\n\\nI just bought the latest model and I'm honestly regretting it. \"\n",
        "     \"The fuel efficiency is far worse than advertised, and the infotainment system is buggy and slow. \"\n",
        "     \"It's been a frustrating experience from day one.\"),\n",
        "\n",
        "    (\"Subject: What a terrible game!\\n\\nI can't believe the team lost again. The pitching was awful and they made so many errors in the field. \"\n",
        "     \"They looked completely unprepared and uninterested. A truly pathetic performance.\"),\n",
        "\n",
        "    (\"Subject: A positive political discussion\\n\\nIt was great to see both sides debating the policy with respect and facts. \"\n",
        "     \"Instead of yelling, they found common ground and focused on what's best for the country. \"\n",
        "     \"I feel optimistic after watching that.\"),\n",
        "\n",
        "    # --- 4. Both Hard to Classify AND Different Sentiment ---\n",
        "    (\"Subject: Religious arguments against space travel are wrong.\\n\\n\"\n",
        "     \"Some people argue that spending money on space exploration is immoral when there is suffering on Earth. \"\n",
        "     \"I find this view incredibly short-sighted. Pushing the boundaries of science and exploring God's universe is a noble goal. \"\n",
        "     \"Their arguments are baseless and frankly, quite annoying.\"),\n",
        "\n",
        "    (\"Subject: The terrible software on my new computer\\n\\nI bought a brand new PC and the hardware is fantastic, but the pre-installed Windows OS is a nightmare. \"\n",
        "     \"It's full of bloatware and ads. I'm so frustrated I'm considering switching to a Mac or Linux.\"),\n",
        "\n",
        "    (\"Subject: For Sale: Broken Electronics Lot\\n\\nSelling a box of broken electronic components for cheap. \"\n",
        "     \"Includes several malfunctioning motherboards, cracked GPUs, and dead power supplies. \"\n",
        "     \"Maybe someone can salvage parts from this pile of junk. No guarantees, sold as-is.\")\n",
        "]\n",
        "\n",
        "# --- How to use them ---\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"--- Analyzing Sample {i+1} ---\")\n",
        "    report = analyze_document(text)\n",
        "    print(report)\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "PrwWBd0g6tKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcPnGXo3628F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}